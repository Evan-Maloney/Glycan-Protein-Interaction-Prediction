{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv, global_add_pool, global_max_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "from rdkit.Chem.rdchem import ChiralType\n",
    "from rdkit.Chem.rdchem import BondType\n",
    "from rdkit.Chem.rdchem import BondStereo\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from typing import Tuple, List, Dict, Optional, Union\n",
    "\n",
    "from lightning.pytorch.utilities.combined_loader import CombinedLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install lightning -c conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Dataset train-val set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "device = torch.device(\"cpu\") # \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cluster_glycans(glycans, radius, fp_size, n_clusters):\n",
    "\n",
    "    def get_morgan_count_fingerprint(smiles, radius, fp_size):\n",
    "        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return {f\"mf_{i}\": 0 for i in range(fp_size)} \n",
    "\n",
    "\n",
    "        #The useChirality parameter in Morgan fingerprints determines whether chirality is considered when encoding a molecule.\n",
    "        #includeChirality=True = Differentiates between enantiomers (model will treat mirror-image molecules as different)\n",
    "        #includeChirality=False = Ignores chirality (model will treat mirror-image molecules as the same)\n",
    "        kid_named_morgan_finger = rdFingerprintGenerator.GetMorganGenerator(radius=radius,fpSize=fp_size, includeChirality=True)\n",
    "\n",
    "        cfp = kid_named_morgan_finger.GetCountFingerprint(mol)  \n",
    "        bit_counts = cfp.GetNonzeroElements()  \n",
    "\n",
    "        # Convert to a full fp_size-length feature vector\n",
    "        fingerprint_vector = {f\"mf_{i}\": bit_counts.get(i, 0) for i in range(fp_size)}\n",
    "        return fingerprint_vector\n",
    "\n",
    "    fingerprint_df = glycans['SMILES'].apply(lambda x: get_morgan_count_fingerprint(x, radius, fp_size)).apply(pd.Series)\n",
    "    \n",
    "    glycans = pd.concat([glycans, fingerprint_df], axis=1)\n",
    "    \n",
    "    # matrix version of fingerprint features. Each row is a glycan, each column is a fingerprint component shape: (611, 2048)\n",
    "    finger_counts_matrix = fingerprint_df.values\n",
    "    # pdist calculates the euclidean distance between the combination of each glycan with every other glycan. Then squareform() turns this into a matrix representation where each row is a glycan and each column is the same list of glycans so we can have a comparison matrix. Shape: (611, 611)\n",
    "    dist_matrix = squareform(pdist(finger_counts_matrix, metric=\"euclidean\"))\n",
    "    \n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(dist_matrix)\n",
    "    \n",
    "    glycans['cluster_label'] = labels\n",
    "    \n",
    "    return glycans\n",
    "\n",
    "def cluster_proteins(proteins, n_clusters):\n",
    "    \n",
    "    \n",
    "    def compute_protein_features(seq):\n",
    "\n",
    "        # Add reasoning for feature vectors\n",
    "        \n",
    "        # Protein Analysis is a Tool from Biopython\n",
    "        analysis = ProteinAnalysis(seq)\n",
    "        features = {}\n",
    "        \n",
    "        # The following are Basic Features\n",
    "        features['length'] = len(seq)\n",
    "        features['mw'] = analysis.molecular_weight()\n",
    "        features['instability_index'] = analysis.instability_index()\n",
    "\n",
    "        features['net_charge_pH7'] = analysis.charge_at_pH(7.0)\n",
    "\n",
    "        aa_percent = analysis.get_amino_acids_percent()\n",
    "\n",
    "        # Prompted ChatGPT to ask how to parse a\n",
    "        # N, Q, S, T: Polar Amino Acids, often involved in hydrogen bonding with glycans\n",
    "        # K, R: Basic Amino Acids, can form hydrogen bonds and electrostatic bonds\n",
    "        # D, E: Acidic Amino Acids, can interact with positively charged groups of glycans\n",
    "        for aa in ['N', 'Q', 'S', 'T', 'K', 'R', 'D', 'E']:\n",
    "            features[f'frac_{aa}'] = aa_percent.get(aa, 0.0)\n",
    "\n",
    "    \n",
    "    # F, Y, W are aromatic amino acids which bind with glycans\n",
    "        for aa in ['F', 'Y', 'W']:\n",
    "            features[f'frac_{aa}'] = aa_percent.get(aa, 0.0)\n",
    "            features['aromatic_binding_score'] = (\n",
    "            aa_percent.get('F', 0.0) +\n",
    "            aa_percent.get('Y', 0.0) +\n",
    "            aa_percent.get('W', 0.0)\n",
    "        )\n",
    "\n",
    "        features['aromaticity'] = analysis.aromaticity()\n",
    "\n",
    "        features['hydrophobicity'] = analysis.gravy()\n",
    "\n",
    "        return features\n",
    "\n",
    "    feature_dicts = proteins['Amino Acid Sequence'].apply(compute_protein_features)\n",
    "    features_df = pd.DataFrame(list(feature_dicts))\n",
    "\n",
    "    proteins = pd.concat([proteins, features_df], axis=1)\n",
    "    \n",
    "    # Select the feature columns (all columns from the feature extraction)\n",
    "    feature_columns = features_df.columns.tolist()\n",
    "    feature_data = proteins[feature_columns].values\n",
    "\n",
    "    # apply k means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    proteins['cluster_label'] = kmeans.fit_predict(feature_data)\n",
    "    \n",
    "    return proteins\n",
    "\n",
    "def stratified_train_test_split(fractions_df, glycans_df, proteins_df, test_size, random_state, mode='AND'):\n",
    "    \"\"\"\n",
    "    Create a stratified train-test split where:\n",
    "    1. Test set has unique GlycanIDs and ProteinGroups not seen in training\n",
    "    2. Distribution of cluster_labels for both glycans and proteins is maintained\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fractions_df : pandas.DataFrame\n",
    "        DataFrame containing ['ObjId', 'ProteinGroup', 'Concentration', 'GlycanID', 'f']\n",
    "    glycans_df : pandas.DataFrame\n",
    "        DataFrame containing ['Name', 'cluster_label'] where Name maps to GlycanID\n",
    "    proteins_df : pandas.DataFrame\n",
    "        DataFrame containing ['ProteinGroup', 'cluster_label']\n",
    "    test_size : float, default=0.1\n",
    "        Proportion of data to include in the test set\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_indices : numpy.ndarray\n",
    "        Indices of fractions_df that belong to the training set\n",
    "    test_indices : numpy.ndarray\n",
    "        Indices of fractions_df that belong to the test set\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Merge cluster labels from glycans and proteins into fractions\n",
    "    fractions_with_clusters = fractions_df.copy()\n",
    "    \n",
    "    # Map glycan cluster labels\n",
    "    glycan_cluster_map = dict(zip(glycans_df['Name'], glycans_df['cluster_label']))\n",
    "    fractions_with_clusters['glycan_cluster'] = fractions_with_clusters['GlycanID'].map(glycan_cluster_map)\n",
    "    \n",
    "    # Map protein cluster labels\n",
    "    protein_cluster_map = dict(zip(proteins_df['ProteinGroup'], proteins_df['cluster_label']))\n",
    "    fractions_with_clusters['protein_cluster'] = fractions_with_clusters['ProteinGroup'].map(protein_cluster_map)\n",
    "    \n",
    "    # Get unique glycans and proteins with their cluster labels\n",
    "    unique_glycans = glycans_df[['Name', 'cluster_label']].drop_duplicates()\n",
    "    unique_proteins = proteins_df[['ProteinGroup', 'cluster_label']].drop_duplicates()\n",
    "    \n",
    "    # Calculate target counts for each cluster in test set\n",
    "    glycan_cluster_counts = unique_glycans['cluster_label'].value_counts().to_dict()\n",
    "    protein_cluster_counts = unique_proteins['cluster_label'].value_counts().to_dict()\n",
    "    \n",
    "    glycan_test_counts = {cluster: max(1, int(np.ceil(count * test_size))) \n",
    "                         for cluster, count in glycan_cluster_counts.items()}\n",
    "    protein_test_counts = {cluster: max(1, int(np.ceil(count * test_size))) \n",
    "                          for cluster, count in protein_cluster_counts.items()}\n",
    "    \n",
    "    # Select glycans and proteins for test set while respecting cluster distributions\n",
    "    test_glycans = []\n",
    "    for cluster, target_count in glycan_test_counts.items():\n",
    "        cluster_glycans = unique_glycans[unique_glycans['cluster_label'] == cluster]['Name'].tolist()\n",
    "        selected = np.random.choice(cluster_glycans, size=min(target_count, len(cluster_glycans)), replace=False)\n",
    "        test_glycans.extend(selected)\n",
    "    \n",
    "    test_proteins = []\n",
    "    for cluster, target_count in protein_test_counts.items():\n",
    "        cluster_proteins = unique_proteins[unique_proteins['cluster_label'] == cluster]['ProteinGroup'].tolist()\n",
    "        selected = np.random.choice(cluster_proteins, size=min(target_count, len(cluster_proteins)), replace=False)\n",
    "        test_proteins.extend(selected)\n",
    "        \n",
    "        \n",
    "    if mode == 'AND':\n",
    "        \n",
    "        is_test = ((fractions_df['GlycanID'].isin(test_glycans)) & \n",
    "                (fractions_df['ProteinGroup'].isin(test_proteins)))\n",
    "\n",
    "        is_train = ((~fractions_df['GlycanID'].isin(test_glycans)) & \n",
    "                        (~fractions_df['ProteinGroup'].isin(test_proteins)))\n",
    "                \n",
    "        test_indices = fractions_df[is_test].index\n",
    "\n",
    "        train_indices = fractions_df[is_train].index\n",
    "        \n",
    "        print(f'-------------Test size (% of glycans and proteins as combinations in test set): {test_size*100}% -------------')\n",
    "\n",
    "        print(f'train size: {len(train_indices)}, test size: {len(test_indices)}, total: {len(fractions_df)}')\n",
    "                \n",
    "        print(f'train size: {round((len(train_indices)/len(fractions_df))*100, 2)}%, test size: {round((len(test_indices)/len(fractions_df))*100, 2)}%')\n",
    "        \n",
    "        print(f'test size % in terms of test/(training+test) size: {round((len(test_indices)/(len(train_indices)+len(test_indices)))*100, 2)}%')\n",
    "        \n",
    "        print(f'Total % of dataset used: {round(((len(train_indices)+len(test_indices))/len(fractions_df))*100, 2)}%\\n')\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        # Create train and test masks\n",
    "        is_test = ((fractions_with_clusters['GlycanID'].isin(test_glycans)) | \n",
    "                (fractions_with_clusters['ProteinGroup'].isin(test_proteins)))\n",
    "        \n",
    "        test_indices = fractions_with_clusters[is_test].index\n",
    "        train_indices = fractions_with_clusters[~is_test].index\n",
    "    \n",
    "    \n",
    "    return train_indices, test_indices\n",
    "\n",
    "def prepare_train_val_datasets(\n",
    "    fractions_df: pd.DataFrame,\n",
    "    glycans_df: pd.DataFrame,\n",
    "    proteins_df: pd.DataFrame,\n",
    "    glycan_encoder,\n",
    "    protein_encoder,\n",
    "    glycan_type: str,\n",
    "    random_state: int,\n",
    "    split_mode: str,\n",
    "    use_kfolds: bool,\n",
    "    k_folds: float,\n",
    "    val_split: float,\n",
    "    device: torch.device\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Prepare train and validation datasets\n",
    "    \n",
    "    Args:\n",
    "        df: Full dataset DataFrame\n",
    "        val_split: Fraction of data to use for validation\n",
    "        glycan_encoder: Encoder for glycans\n",
    "        protein_encoder: Encoder for proteins\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of train and validation datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    # for each glycan create a glycan_encoding feature where we use glycan_encoder to encode the SMILES\n",
    "    # for each protein create a protein_encoding feature where we use protein_encoder to encode the aminoacids\n",
    "    glycan_encodings = glycan_encoder.encode_batch(glycans_df[glycan_type].tolist(), device)\n",
    "    protein_encodings = protein_encoder.encode_batch(proteins_df['Amino Acid Sequence'].tolist(), device)\n",
    "    \n",
    "    \n",
    "    # Might move to config but leave for now as our train and test are clusterd and stratified using these parameters\n",
    "    radius = 3\n",
    "    fp_size = 1024\n",
    "    n_clusters = 3\n",
    "    glycans_df = cluster_glycans(glycans_df, radius, fp_size, n_clusters)\n",
    "    \n",
    "    n_protein_clusters = 3\n",
    "    proteins_df = cluster_proteins(proteins_df, n_protein_clusters)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_indicies, test_indicies = stratified_train_test_split(fractions_df, glycans_df, proteins_df, val_split, random_state, split_mode)\n",
    "    # convert to kfold format so we can use the same code\n",
    "    full_indicies = [(train_indicies, test_indicies)]\n",
    "    \n",
    "    return full_indicies, glycan_encodings, protein_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glycan Encoder classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNGlycanEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim: int = 256, hidden_channels: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Node features (9-dimensional as shown in the figure)\n",
    "        self.node_features = [\n",
    "            'atomic_num',      # Atomic number\n",
    "            'chirality',       # Chirality (important for glycans)\n",
    "            'degree',          # Degree (number of bonds)\n",
    "            'formal_charge',   # Formal charge\n",
    "            'numH',            # Number of hydrogens\n",
    "            'number_radical_e', # Number of radical electrons\n",
    "            'hybridization',   # Hybridization type\n",
    "            'is_aromatic',     # Is the atom aromatic (boolean)\n",
    "            'is_in_ring'       # Is the atom in a ring (boolean)\n",
    "        ]\n",
    "        \n",
    "        # Edge features (3-dimensional as shown in the figure)\n",
    "        self.edge_features = [\n",
    "            'bond_type',         # Type of bond (single, double, etc.)\n",
    "            'stereo_configuration', # Stereo configuration\n",
    "            'is_conjugated'      # Is the bond conjugated (boolean)\n",
    "        ]\n",
    "        \n",
    "        # Define normalization parameters (to be populated during preprocessing)\n",
    "        self.scalers = {}\n",
    "        \n",
    "        # Define GNN layers\n",
    "        self.conv1 = GCNConv(9, hidden_channels//2)  # 9 is the expanded node features\n",
    "        self.conv2 = GCNConv(hidden_channels//2, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels*2)\n",
    "        \n",
    "        # Output layer\n",
    "        self.linear = torch.nn.Linear(hidden_channels*2, embedding_dim)\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "    \n",
    "    def _get_atom_features(self, atom) -> List[float]:\n",
    "        \"\"\"Extract atom features according to the predefined list\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # atomic_num\n",
    "        features.append(atom.GetAtomicNum())\n",
    "        \n",
    "        # chirality\n",
    "        chirality_type = int(atom.GetChiralTag())\n",
    "        features.append(chirality_type)\n",
    "        \n",
    "        # degree\n",
    "        features.append(atom.GetDegree())\n",
    "        \n",
    "        # formal_charge\n",
    "        features.append(atom.GetFormalCharge())\n",
    "        \n",
    "        # numH\n",
    "        features.append(atom.GetTotalNumHs())\n",
    "        \n",
    "        # number_radical_e\n",
    "        features.append(atom.GetNumRadicalElectrons())\n",
    "        \n",
    "        # hybridization\n",
    "        hybridization_type = int(atom.GetHybridization())\n",
    "        features.append(hybridization_type)\n",
    "        \n",
    "        # is_aromatic\n",
    "        features.append(1 if atom.GetIsAromatic() else 0)\n",
    "        \n",
    "        # is_in_ring\n",
    "        features.append(1 if atom.IsInRing() else 0)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _get_bond_features(self, bond) -> List[float]:\n",
    "        \"\"\"Extract bond features according to the predefined list\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # bond_type\n",
    "        bond_type = int(bond.GetBondType())\n",
    "        features.append(bond_type)\n",
    "        \n",
    "        # stereo_configuration\n",
    "        stereo = int(bond.GetStereo())\n",
    "        features.append(stereo)\n",
    "        \n",
    "        # is_conjugated\n",
    "        features.append(1 if bond.GetIsConjugated() else 0)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _mol_to_graph_data(self, mol) -> Data:\n",
    "        \"\"\"Convert an RDKit molecule to a PyTorch Geometric Data object\"\"\"\n",
    "        # Get atom features\n",
    "        node_features = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            node_features.append(self._get_atom_features(atom))\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "        \n",
    "        # Get edge indices and features\n",
    "        edge_indices = []\n",
    "        edge_features = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            \n",
    "            edge_indices.append([i, j])\n",
    "            edge_indices.append([j, i])  # Add reverse edge for undirected graph\n",
    "            \n",
    "            features = self._get_bond_features(bond)\n",
    "            edge_features.append(features)\n",
    "            edge_features.append(features)  # Duplicate for reverse edge\n",
    "        \n",
    "        if len(edge_indices) > 0:\n",
    "            edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "            edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "        else:\n",
    "            # Handle molecules with no bonds (rare case)\n",
    "            edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "            edge_attr = torch.zeros((0, 3), dtype=torch.float)\n",
    "        \n",
    "        # Create and normalize features\n",
    "        x_norm = self._normalize_node_features(x)\n",
    "        edge_attr_norm = self._normalize_edge_features(edge_attr)\n",
    "        \n",
    "        # Create the PyTorch Geometric Data object\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            x_norm=x_norm,\n",
    "            edge_attr_norm=edge_attr_norm\n",
    "        )\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _normalize_node_features(self, x):\n",
    "        \"\"\"Apply normalization and one-hot encoding to node features\"\"\"\n",
    "        # This is a placeholder - in production you'd use the scalers and encoding logic\n",
    "        # from your preprocessing code\n",
    "        # For simplicity, we're just returning the raw features\n",
    "        return x\n",
    "    \n",
    "    def _normalize_edge_features(self, edge_attr):\n",
    "        \"\"\"Apply normalization and one-hot encoding to edge features\"\"\"\n",
    "        # This is a placeholder - in production you'd use the scalers and encoding logic\n",
    "        # from your preprocessing code\n",
    "        # For simplicity, we're just returning the raw features\n",
    "        return edge_attr\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \"\"\"Process a batch of molecular graphs through the GNN\"\"\"\n",
    "        x, edge_index, batch = data.x_norm, data.edge_index, data.batch\n",
    "        \n",
    "        # Apply GNN layers\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        \n",
    "        # Global pooling to get a graph-level representation\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        x = F.dropout(x, p=0.5)\n",
    "        \n",
    "        # Final projection to embedding dimension\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def encode_iupac(self, iupac_str: str, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"aaaaaa\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def encode_smiles(self, smiles: str, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"Convert a SMILES string to a graph embedding\"\"\"\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            raise ValueError(f\"Could not parse SMILES: {smiles}\")\n",
    "        \n",
    "        # Optionally add hydrogen atoms\n",
    "        mol = Chem.AddHs(mol)\n",
    "        \n",
    "        # Convert to a graph data object\n",
    "        data = self._mol_to_graph_data(mol)\n",
    "        \n",
    "        # Create a batch with just this single molecule\n",
    "        data.batch = torch.zeros(data.x.size(0), dtype=torch.long)\n",
    "        \n",
    "        # Move to device\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # Get embedding\n",
    "        with torch.no_grad():\n",
    "            embedding = self.forward(data)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def encode_batch(self, batch_data: List[str], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"Convert a batch of SMILES strings to graph embeddings\"\"\"\n",
    "        # Process each molecule individually\n",
    "        batch_embeddings = []\n",
    "        for smiles in batch_data:\n",
    "            embedding = self.encode_smiles(smiles, device)\n",
    "            batch_embeddings.append(embedding)\n",
    "        #for iupac in batch_data:\n",
    "            #embedding = self.encode_iupac(iupac, device)\n",
    "            #batch_embeddings.append(embedding)\n",
    "        \n",
    "        # Stack all embeddings\n",
    "        batch = torch.cat(batch_embeddings, dim=0)\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def preprocess_dataset(self, smiles_list: List[str]):\n",
    "        \"\"\"Precompute normalization parameters for the dataset\"\"\"\n",
    "        # Convert all molecules to graphs and collect statistics\n",
    "        all_node_features = []\n",
    "        all_edge_features = []\n",
    "        \n",
    "        for smiles in smiles_list:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                continue\n",
    "                \n",
    "            # Add hydrogens\n",
    "            mol = Chem.AddHs(mol)\n",
    "            \n",
    "            # Collect node features\n",
    "            for atom in mol.GetAtoms():\n",
    "                features = self._get_atom_features(atom)\n",
    "                all_node_features.append(features)\n",
    "            \n",
    "            # Collect edge features\n",
    "            for bond in mol.GetBonds():\n",
    "                features = self._get_bond_features(bond)\n",
    "                all_edge_features.append(features)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        all_node_features = torch.tensor(all_node_features, dtype=torch.float)\n",
    "        all_edge_features = torch.tensor(all_edge_features, dtype=torch.float)\n",
    "        \n",
    "        # Compute normalization parameters\n",
    "        for i, feature_name in enumerate(self.node_features):\n",
    "            feature_values = all_node_features[:, i]\n",
    "            self.scalers[feature_name] = {\n",
    "                'min': float(feature_values.min()),\n",
    "                'max': float(feature_values.max()),\n",
    "                'mean': float(feature_values.mean()),\n",
    "                'std': float(feature_values.std())\n",
    "            }\n",
    "        \n",
    "        for i, feature_name in enumerate(self.edge_features):\n",
    "            feature_values = all_edge_features[:, i]\n",
    "            self.scalers[feature_name] = {\n",
    "                'min': float(feature_values.min()),\n",
    "                'max': float(feature_values.max()),\n",
    "                'mean': float(feature_values.mean()),\n",
    "                'std': float(feature_values.std())\n",
    "            }\n",
    "        \n",
    "        return self.scalers\n",
    "    \n",
    "    @property\n",
    "    def embedding_dim(self) -> int:\n",
    "        return self._embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protein encoder classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedGNNProteinEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced Graph Neural Network-based Protein Encoder that incorporates:\n",
    "    - Rich amino acid feature representation\n",
    "    - Flexible graph structures (sequential, predicted contacts)\n",
    "    - Attention-based message passing\n",
    "    - Multiple readout functions\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 embedding_dim: int = 256, \n",
    "                 hidden_channels: int = 128,\n",
    "                 num_layers: int = 3,\n",
    "                 dropout: float = 0.2,\n",
    "                 use_attention: bool = True,\n",
    "                 readout_mode: str = 'mean'):\n",
    "        \"\"\"\n",
    "        Initialize the advanced GNN protein encoder.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Final embedding dimension\n",
    "            hidden_channels: Size of hidden layers in GNN\n",
    "            num_layers: Number of GNN layers\n",
    "            dropout: Dropout probability\n",
    "            use_attention: Whether to use attention-based message passing\n",
    "            readout_mode: Method for graph-level pooling ('mean', 'sum', 'max', 'mean+max')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_attention = use_attention\n",
    "        self.readout_mode = readout_mode\n",
    "        \n",
    "        # Feature dimensions\n",
    "        self.aa_embedding_dim = 20  # One-hot encoding of amino acids\n",
    "        self.physicochemical_dim = 12  # Various amino acid properties\n",
    "        self.position_embedding_dim = 16  # Positional encoding\n",
    "        \n",
    "        # Total node feature dimension\n",
    "        node_feature_dim = self.aa_embedding_dim + self.physicochemical_dim + self.position_embedding_dim\n",
    "        \n",
    "        # Amino acid mappings\n",
    "        self.aa_to_idx = {aa: i for i, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
    "        self.default_idx = len(self.aa_to_idx)  # For unknown amino acids\n",
    "        \n",
    "        # Feature initialization layers\n",
    "        self.position_embedding = nn.Embedding(1000, self.position_embedding_dim)  # Max sequence length of 1000\n",
    "        \n",
    "        # Physicochemical property mappings (pre-computed)\n",
    "        self.aa_properties = self._initialize_aa_properties()\n",
    "        \n",
    "        # GNN layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        \n",
    "        # First layer takes the combined node features\n",
    "        if use_attention:\n",
    "            self.convs.append(GATConv(node_feature_dim, hidden_channels, heads=4, concat=False))\n",
    "        else:\n",
    "            self.convs.append(GCNConv(node_feature_dim, hidden_channels))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_channels))\n",
    "        \n",
    "        # Additional layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            if use_attention:\n",
    "                self.convs.append(GATConv(hidden_channels, hidden_channels, heads=4, concat=False))\n",
    "            else:\n",
    "                self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_channels))\n",
    "        \n",
    "        # Output projections depend on readout mode\n",
    "        output_dim = hidden_channels if 'mean+max' not in readout_mode else hidden_channels * 2\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def _initialize_aa_properties(self) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Initialize physicochemical properties for each amino acid\"\"\"\n",
    "        properties = {}\n",
    "        \n",
    "        # These values are based on common AA properties: \n",
    "        # hydrophobicity, charge, size, polarity, etc.\n",
    "        \n",
    "        # Define key properties for each amino acid (normalized)\n",
    "        # Format: [hydrophobicity, charge, size, polarity, aromaticity, \n",
    "        #          h-bond donor, h-bond acceptor, pKa, pI, flexibility,\n",
    "        #          reactivity, glycosylation_site]\n",
    "        \n",
    "        properties['A'] = torch.tensor([0.7, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.3, 0.1, 0.0])\n",
    "        properties['C'] = torch.tensor([0.8, 0.0, 0.2, 0.1, 0.0, 0.5, 0.0, 0.9, 0.4, 0.2, 0.9, 0.0])\n",
    "        properties['D'] = torch.tensor([0.3, -1.0, 0.3, 0.9, 0.0, 0.0, 1.0, 0.1, 0.3, 0.5, 0.4, 0.0])\n",
    "        properties['E'] = torch.tensor([0.4, -1.0, 0.4, 0.8, 0.0, 0.0, 1.0, 0.2, 0.3, 0.5, 0.3, 0.0])\n",
    "        properties['F'] = torch.tensor([0.9, 0.0, 0.6, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.2, 0.2, 0.0])\n",
    "        properties['G'] = torch.tensor([0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.2, 0.0])\n",
    "        properties['H'] = torch.tensor([0.5, 0.5, 0.5, 0.7, 0.5, 0.5, 0.5, 0.6, 0.7, 0.3, 0.6, 0.0])\n",
    "        properties['I'] = torch.tensor([1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.1, 0.1, 0.0])\n",
    "        properties['K'] = torch.tensor([0.3, 1.0, 0.6, 0.8, 0.0, 0.5, 0.0, 1.0, 0.9, 0.5, 0.3, 0.0])\n",
    "        properties['L'] = torch.tensor([0.9, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.2, 0.1, 0.0])\n",
    "        properties['M'] = torch.tensor([0.7, 0.0, 0.5, 0.1, 0.0, 0.0, 0.0, 0.0, 0.5, 0.3, 0.2, 0.0])\n",
    "        properties['N'] = torch.tensor([0.3, 0.0, 0.3, 0.8, 0.0, 0.5, 0.5, 0.0, 0.5, 0.5, 0.3, 1.0])\n",
    "        properties['P'] = torch.tensor([0.5, 0.0, 0.3, 0.3, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.2, 0.0])\n",
    "        properties['Q'] = torch.tensor([0.4, 0.0, 0.4, 0.7, 0.0, 0.5, 0.5, 0.0, 0.5, 0.4, 0.2, 0.0])\n",
    "        properties['R'] = torch.tensor([0.2, 1.0, 0.7, 0.9, 0.0, 0.5, 0.0, 0.5, 1.0, 0.4, 0.3, 0.0])\n",
    "        properties['S'] = torch.tensor([0.4, 0.0, 0.2, 0.6, 0.0, 0.5, 0.5, 0.0, 0.5, 0.6, 0.2, 0.5])\n",
    "        properties['T'] = torch.tensor([0.5, 0.0, 0.3, 0.5, 0.0, 0.5, 0.5, 0.0, 0.5, 0.4, 0.2, 0.5])\n",
    "        properties['V'] = torch.tensor([0.8, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.2, 0.1, 0.0])\n",
    "        properties['W'] = torch.tensor([0.6, 0.0, 0.8, 0.1, 1.0, 0.5, 0.0, 0.0, 0.5, 0.1, 0.2, 0.0])\n",
    "        properties['Y'] = torch.tensor([0.7, 0.0, 0.7, 0.4, 0.8, 0.5, 0.5, 0.3, 0.5, 0.2, 0.3, 0.0])\n",
    "        \n",
    "        # Default for unknown amino acids (average values)\n",
    "        properties['X'] = torch.mean(torch.stack([prop for prop in properties.values()]), dim=0)\n",
    "        \n",
    "        return properties\n",
    "        \n",
    "    def _one_hot_encode_aa(self, aa: str) -> torch.Tensor:\n",
    "        \"\"\"One-hot encode an amino acid\"\"\"\n",
    "        idx = self.aa_to_idx.get(aa, self.default_idx)\n",
    "        one_hot = torch.zeros(self.aa_embedding_dim)\n",
    "        if idx < self.aa_embedding_dim:\n",
    "            one_hot[idx] = 1.0\n",
    "        return one_hot\n",
    "    \n",
    "    def _get_aa_properties(self, aa: str) -> torch.Tensor:\n",
    "        \"\"\"Get physicochemical properties for an amino acid\"\"\"\n",
    "        return self.aa_properties.get(aa, self.aa_properties['X'])\n",
    "    \n",
    "    def _sequence_to_graph(self, \n",
    "                          sequence: str, \n",
    "                          contact_map: Optional[Union[torch.Tensor, List, np.ndarray, None]] = None,\n",
    "                          distance_threshold: float = 8.0) -> Data:\n",
    "        \"\"\"\n",
    "        Convert a protein sequence to a graph representation.\n",
    "        \n",
    "        Args:\n",
    "            sequence: Amino acid sequence\n",
    "            contact_map: Optional tensor of pairwise distances/contacts\n",
    "            distance_threshold: Threshold for considering residues in contact\n",
    "            \n",
    "        Returns:\n",
    "            PyTorch Geometric Data object\n",
    "        \"\"\"\n",
    "        # Node features: combine one-hot encoding, properties, and position\n",
    "        x = []\n",
    "        for i, aa in enumerate(sequence):\n",
    "            if aa not in self.aa_to_idx and aa != 'X':\n",
    "                aa = 'X'  # Use default for unknown amino acids\n",
    "                \n",
    "            # Combine features\n",
    "            one_hot = self._one_hot_encode_aa(aa)\n",
    "            properties = self._get_aa_properties(aa)\n",
    "            position = self.position_embedding(torch.tensor([min(i, 999)]))\n",
    "            \n",
    "            # Concatenate all features\n",
    "            features = torch.cat([one_hot, properties, position.squeeze(0)])\n",
    "            x.append(features)\n",
    "            \n",
    "        # Create node features tensor\n",
    "        x = torch.stack(x)\n",
    "        \n",
    "        # Create edge index\n",
    "        edge_index = []\n",
    "        \n",
    "        # Add sequential connections (each AA connected to neighbors within window)\n",
    "        window_size = 3  # Connect each AA to this many neighbors in each direction\n",
    "        for i in range(len(sequence)):\n",
    "            # Connect to previous AAs within window\n",
    "            for w in range(1, window_size + 1):\n",
    "                if i - w >= 0:\n",
    "                    edge_index.append([i-w, i])\n",
    "                    edge_index.append([i, i-w])  # Bidirectional\n",
    "            \n",
    "            # Connect to next AAs within window\n",
    "            for w in range(1, window_size + 1):\n",
    "                if i + w < len(sequence):\n",
    "                    edge_index.append([i, i+w])\n",
    "                    edge_index.append([i+w, i])  # Bidirectional\n",
    "        \n",
    "        # Add contacts from contact map if provided\n",
    "        if contact_map is not None:\n",
    "            try:\n",
    "                # Convert to tensor if not already\n",
    "                if not isinstance(contact_map, torch.Tensor):\n",
    "                    if isinstance(contact_map, np.ndarray):\n",
    "                        contact_map = torch.tensor(contact_map)\n",
    "                    elif isinstance(contact_map, list):\n",
    "                        contact_map = torch.tensor(contact_map)\n",
    "                \n",
    "                # Only use contact map if it's now a tensor with the right shape\n",
    "                if isinstance(contact_map, torch.Tensor) and contact_map.dim() == 2:\n",
    "                    for i in range(len(sequence)):\n",
    "                        for j in range(i + window_size + 1, min(len(sequence), contact_map.shape[0])):\n",
    "                            # Check dimensions to avoid index errors\n",
    "                            if i < contact_map.shape[0] and j < contact_map.shape[1]:\n",
    "                                if contact_map[i, j] <= distance_threshold:\n",
    "                                    edge_index.append([i, j])\n",
    "                                    edge_index.append([j, i])  # Bidirectional\n",
    "            except Exception as e:\n",
    "                # If we encounter any error with the contact map, just ignore it\n",
    "                print(f\"Warning: Could not use contact map: {e}\")\n",
    "        \n",
    "        # Create edge index tensor\n",
    "        if edge_index:\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        else:\n",
    "            # Handle case with no edges (very short sequence)\n",
    "            edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "        \n",
    "        # Create PyG Data object\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        return data\n",
    "    \n",
    "    def forward(self, data: Data) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Process protein graph through the GNN.\n",
    "        \n",
    "        Args:\n",
    "            data: PyTorch Geometric Data object\n",
    "            \n",
    "        Returns:\n",
    "            Protein embedding tensor\n",
    "        \"\"\"\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # Apply GNN layers with residual connections\n",
    "        for i in range(self.num_layers):\n",
    "            identity = x\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout)#, training=self.training)\n",
    "            \n",
    "            # Add residual connection if dimensions match\n",
    "            if i > 0 and x.size(-1) == identity.size(-1):\n",
    "                x = x + identity\n",
    "        \n",
    "        # Different pooling strategies\n",
    "        if self.readout_mode == 'mean':\n",
    "            x = global_mean_pool(x, batch)\n",
    "        elif self.readout_mode == 'sum':\n",
    "            x = global_add_pool(x, batch)\n",
    "        elif self.readout_mode == 'max':\n",
    "            # Manual implementation of max pooling\n",
    "            x_max, _ = global_max_pool(x, batch, dim=0)\n",
    "            x = x_max\n",
    "        elif self.readout_mode == 'mean+max':\n",
    "            x_mean = global_mean_pool(x, batch)\n",
    "            # Manual implementation of max pooling\n",
    "            x_max, _ = global_max_pool(x, batch, dim=0)\n",
    "            x = torch.cat([x_mean, x_max], dim=1)\n",
    "        \n",
    "        # Final projection\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def encode_sequence(self, \n",
    "                         sequence: str, \n",
    "                         device: Optional[torch.device] = None,\n",
    "                         contact_map: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode a single protein sequence.\n",
    "        \n",
    "        Args:\n",
    "            sequence: Amino acid sequence\n",
    "            contact_map: Optional contact map for the protein\n",
    "            \n",
    "        Returns:\n",
    "            Embedding tensor\n",
    "        \"\"\"\n",
    "        # Convert sequence to graph\n",
    "        data = self._sequence_to_graph(sequence, contact_map)\n",
    "        \n",
    "        # Add batch dimension for single sequence\n",
    "        data.batch = torch.zeros(len(sequence), dtype=torch.long)\n",
    "        \n",
    "        # Move to device if specified\n",
    "        if device is not None:\n",
    "            data = data.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            embedding = self.forward(data)\n",
    "            \n",
    "        return embedding\n",
    "    \n",
    "    def encode_batch(self, \n",
    "                     batch_data: List[str],\n",
    "                     device: torch.device = None,\n",
    "                     contact_maps: Optional[List[torch.Tensor]] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode a batch of protein sequences.\n",
    "        \n",
    "        Args:\n",
    "            batch_data: List of amino acid sequences\n",
    "            device: Device to place tensors on\n",
    "            contact_maps: Optional list of contact maps for each protein\n",
    "            \n",
    "        Returns:\n",
    "            Batch of embedding tensors\n",
    "        \"\"\"\n",
    "        # Create a list of Data objects\n",
    "        data_list = []\n",
    "        for sequence in batch_data:\n",
    "            # Don't use contact maps for now to avoid the error\n",
    "            data = self._sequence_to_graph(sequence, None)\n",
    "            data_list.append(data)\n",
    "            \n",
    "        # Create a batch from the list\n",
    "        batch = Batch.from_data_list(data_list)\n",
    "        \n",
    "        # Move to device if specified\n",
    "        if device is not None:\n",
    "            batch = batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.forward(batch)\n",
    "            \n",
    "        return embeddings\n",
    "    \n",
    "    def predict_secondary_structure(self, sequence: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Predict secondary structure probabilities (helix, sheet, coil)\n",
    "        \n",
    "        Args:\n",
    "            sequence: Amino acid sequence\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of secondary structure probabilities\n",
    "        \"\"\"\n",
    "        # This would require a separate prediction head\n",
    "        # Here we use Biopython as a placeholder\n",
    "        try:\n",
    "            analysis = ProteinAnalysis(sequence)\n",
    "            helix, turn, sheet = analysis.secondary_structure_fraction()\n",
    "            \n",
    "            # Convert to tensor format that could come from a model\n",
    "            ss_pred = {\n",
    "                'helix': torch.tensor([helix] * len(sequence)),\n",
    "                'sheet': torch.tensor([sheet] * len(sequence)),\n",
    "                'coil': torch.tensor([turn] * len(sequence))\n",
    "            }\n",
    "            return ss_pred\n",
    "        except:\n",
    "            # Default values if analysis fails\n",
    "            return {\n",
    "                'helix': torch.zeros(len(sequence)),\n",
    "                'sheet': torch.zeros(len(sequence)),\n",
    "                'coil': torch.ones(len(sequence))\n",
    "            }\n",
    "    \n",
    "    def estimate_contact_map(self, sequence: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Estimate a contact map based on amino acid properties and sequential distance.\n",
    "        This is a placeholder - ideally a dedicated contact prediction model would be used.\n",
    "        \n",
    "        Args:\n",
    "            sequence: Amino acid sequence\n",
    "            \n",
    "        Returns:\n",
    "            Estimated contact map (distances between residues)\n",
    "        \"\"\"\n",
    "        seq_len = len(sequence)\n",
    "        contact_map = torch.ones(seq_len, seq_len) * 100  # Initialize with large distances\n",
    "        \n",
    "        # Set sequential distances\n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                # Sequential distance penalty\n",
    "                contact_map[i, j] = min(contact_map[i, j], abs(i - j) * 3.8)\n",
    "                \n",
    "                # Reduce distance for hydrophobic interactions\n",
    "                aa_i = sequence[i] if sequence[i] in self.aa_to_idx else 'X'\n",
    "                aa_j = sequence[j] if sequence[j] in self.aa_to_idx else 'X'\n",
    "                hydrophobicity_i = self.aa_properties[aa_i][0]\n",
    "                hydrophobicity_j = self.aa_properties[aa_j][0]\n",
    "                \n",
    "                # Hydrophobic residues tend to cluster\n",
    "                if hydrophobicity_i > 0.7 and hydrophobicity_j > 0.7:\n",
    "                    contact_map[i, j] = min(contact_map[i, j], 8.0 + abs(i - j) * 0.5)\n",
    "                \n",
    "                # Ionic interactions between charged residues\n",
    "                charge_i = self.aa_properties[aa_i][1]\n",
    "                charge_j = self.aa_properties[aa_j][1]\n",
    "                if abs(i - j) > 4 and charge_i * charge_j < 0:  # Opposite charges attract\n",
    "                    contact_map[i, j] = min(contact_map[i, j], 10.0)\n",
    "                    \n",
    "        return contact_map\n",
    "    \n",
    "    @property\n",
    "    def embedding_dim(self) -> int:\n",
    "        return self._embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlycoProteinDataset(Dataset):\n",
    "    def __init__(self, fractions_df, glycan_encodings, protein_encodings, glycan_mapping, protein_mapping, task_id):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fractions_df: DataFrame with fraction data\n",
    "            glycan_encodings: Tensor of shape [n_glycans, embedding_dim]\n",
    "            protein_encodings: Tensor of shape [n_proteins, embedding_dim]\n",
    "            glycan_mapping: Dict mapping glycan IDs to indices in glycan_encodings\n",
    "            protein_mapping: Dict mapping protein IDs to indices in protein_encodings\n",
    "        \"\"\"\n",
    "        self.fractions_df = fractions_df\n",
    "        self.glycan_encodings = glycan_encodings\n",
    "        self.protein_encodings = protein_encodings\n",
    "        self.glycan_mapping = glycan_mapping\n",
    "        self.protein_mapping = protein_mapping\n",
    "        self.task_id = task_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fractions_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.fractions_df.iloc[idx]\n",
    "        \n",
    "        # Get the corresponding encodings using the mappings\n",
    "        glycan_idx = self.glycan_mapping[row['GlycanID']]\n",
    "        protein_idx = self.protein_mapping[row['ProteinGroup']]\n",
    "        \n",
    "        return {\n",
    "            'glycan_encoding': self.glycan_encodings[glycan_idx],\n",
    "            'protein_encoding': self.protein_encodings[protein_idx],\n",
    "            'concentration': torch.tensor([row['Concentration']], dtype=torch.float32),\n",
    "            'target': torch.tensor([row['f']], dtype=torch.float32),\n",
    "            'task_id': self.task_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Test size (% of glycans and proteins as combinations in test set): 50.0% -------------\n",
      "train size: 29582, test size: 8054, total: 68492\n",
      "train size: 43.19%, test size: 11.76%\n",
      "test size % in terms of test/(training+test) size: 21.4%\n",
      "Total % of dataset used: 54.95%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/test_env/lib/python3.12/site-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "fractions_df = pd.read_csv('../pipeline/data/Train_Fractions.csv', sep='\\t')\n",
    "glycans_df = pd.read_csv('../pipeline/data/Glycan-Structures-CFG611.txt', sep='\\t')\n",
    "proteins_df = pd.read_csv('../pipeline/data/Protein-Sequence-Table.txt', sep='\\t')\n",
    "\n",
    "glycan_encoder = GNNGlycanEncoder().to(device)\n",
    "protein_encoder = AdvancedGNNProteinEncoder().to(device)\n",
    "\n",
    "glycan_type = 'SMILES'\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "split_mode = 'AND'\n",
    "use_kfolds = False\n",
    "k_folds = 0\n",
    "val_split = 0.5\n",
    "device = 'cpu'\n",
    "\n",
    "full_indicies, glycan_encodings, protein_encodings = prepare_train_val_datasets(fractions_df, glycans_df, proteins_df, glycan_encoder, protein_encoder, glycan_type, random_state, split_mode, use_kfolds, k_folds, val_split, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first task OUR task\n",
    "TASK_ID = 0\n",
    "\n",
    "glycan_mapping = {name: idx for idx, name in enumerate(glycans_df['Name'])}\n",
    "protein_mapping = {name: idx for idx, name in enumerate(proteins_df['ProteinGroup'])}\n",
    "\n",
    "train_idx, val_idx = full_indicies[0]\n",
    "\n",
    "train_data = fractions_df.loc[train_idx]\n",
    "val_data = fractions_df.loc[val_idx]\n",
    "\n",
    "train_pytorch_dataset = GlycoProteinDataset(\n",
    "    train_data, glycan_encodings, protein_encodings, glycan_mapping, protein_mapping, TASK_ID\n",
    ")\n",
    "val_pytorch_dataset = GlycoProteinDataset(\n",
    "    val_data, glycan_encodings, protein_encodings, glycan_mapping, protein_mapping, TASK_ID\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "task469_train_loader = DataLoader(\n",
    "    train_pytorch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "task469_val_loader = DataLoader(\n",
    "    val_pytorch_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GlycanML Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glycanML dataset\n",
    "\n",
    "glyML_TASK_ID = 1\n",
    "\n",
    "glyML_fractions_train_df = pd.read_csv('../pipeline/data/GlycanML/train_fractions.tsv', sep='\\t')\n",
    "glyML_fractions_test_df = pd.read_csv('../pipeline/data/GlycanML/test_fractions.tsv', sep='\\t')\n",
    "glyML_glycans_df = pd.read_csv('../pipeline/data/GlycanML/glycans.tsv', sep='\\t')\n",
    "glyML_proteins_df = pd.read_csv('../pipeline/data/GlycanML/proteins.tsv', sep='\\t')\n",
    "\n",
    "glyML_glycan_mapping = {name: idx for idx, name in enumerate(glyML_glycans_df['Name'])}\n",
    "glyML_protein_mapping = {name: idx for idx, name in enumerate(glyML_proteins_df['ProteinGroup'])}\n",
    "\n",
    "# dont need this I dont think as not doing train-val split\n",
    "#train_data = fractions_df.loc[train_idx]\n",
    "#val_data = fractions_df.loc[val_idx]\n",
    "\n",
    "glyML_glycan_type = 'SMILES'\n",
    "\n",
    "# use the same glycan and protein encoders as for MultiTask we share features and then have diff classif heads\n",
    "glyML_glycan_encodings = glycan_encoder.encode_batch(glyML_glycans_df[glyML_glycan_type].tolist(), device)\n",
    "glyML_protein_encodings = protein_encoder.encode_batch(glyML_proteins_df['Amino Acid Sequence'].tolist(), device)\n",
    "\n",
    "glyML_train_pytorch_dataset = GlycoProteinDataset(\n",
    "    glyML_fractions_train_df, glyML_glycan_encodings, glyML_protein_encodings, glyML_glycan_mapping, glyML_protein_mapping, glyML_TASK_ID\n",
    ")\n",
    "glyML_val_pytorch_dataset = GlycoProteinDataset(\n",
    "    val_data, glyML_glycan_encodings, glyML_protein_encodings, glyML_glycan_mapping, glyML_protein_mapping, glyML_TASK_ID\n",
    ")\n",
    "\n",
    "glyML_batch_size = 32\n",
    "\n",
    "glyML_train_loader = DataLoader(\n",
    "    glyML_train_pytorch_dataset,\n",
    "    batch_size=glyML_batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "glyML_val_loader = DataLoader(\n",
    "    glyML_val_pytorch_dataset,\n",
    "    batch_size=glyML_batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save so we dont have to spend 10 mins each time recalculating this\n",
    "with open('glyML_train_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(glyML_train_pytorch_dataset, f)\n",
    "\n",
    "with open('glyML_val_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(glyML_val_pytorch_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to reload them\n",
    "with open('glyML_train_dataset.pkl', 'rb') as f:\n",
    "    glyML_train_pytorch_dataset = pickle.load(f)\n",
    "\n",
    "with open('glyML_val_dataset.pkl', 'rb') as f:\n",
    "    glyML_val_pytorch_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiTask Classifier network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiTask_Network(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim: int,\n",
    "                 task_output_dims: Dict[int, int],  # Key: task_id, Value: output_dim\n",
    "                 hidden_dims: List[int] = [256, 128, 64]):  # DNN hidden layer sizes tried: #[256, 128, 128, 64, 32]\n",
    "        super(MultiTask_Network, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.task_output_dims = task_output_dims\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        # Shared DNN Layers (based on your DNNBindingPredictor)\n",
    "        dnn_layers = []\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            dnn_layers.append(nn.Linear(input_dim if i == 0 else hidden_dims[i - 1], hidden_dim))\n",
    "            dnn_layers.append(nn.ReLU())\n",
    "            dnn_layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            dnn_layers.append(nn.Dropout(0.4))\n",
    "        self.dnn = nn.Sequential(*dnn_layers)  # Store it as self.dnn\n",
    "\n",
    "        # Task-Specific Output Layers (store in a dictionary)\n",
    "        self.final_layers = nn.ModuleDict({\n",
    "            str(task_id): nn.Linear(hidden_dims[-1], output_dim) # Use hidden_dims[-1] as input\n",
    "            for task_id, output_dim in task_output_dims.items()\n",
    "        })\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, task_id: int):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Args:\n",
    "            x: Input tensor (concatenated glycan/protein embeddings)\n",
    "            task_id: Integer identifying the task.\n",
    "        Returns:\n",
    "            Output tensor for the specified task.\n",
    "        \"\"\"\n",
    "\n",
    "        # Pass through shared DNN layers\n",
    "        x = self.dnn(x) #  Pass input through the DNN\n",
    "\n",
    "        # Task-specific output layer\n",
    "        task_id_str = str(task_id) # crucial to make it a string\n",
    "        if task_id_str in self.final_layers:\n",
    "            x = self.final_layers[task_id_str](x) # Apply the output layer\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid task_id: {task_id}.  Available task_ids are {self.final_layers.keys()}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions: torch.Tensor, targets: torch.Tensor) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate training/validation metrics\n",
    "    \n",
    "    Args:\n",
    "        predictions (torch.Tensor): Model predictions\n",
    "        targets (torch.Tensor): True values\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary of metric names and values\n",
    "    \"\"\"\n",
    "    # convert values to numpy arrays\n",
    "    preds_np = predictions.detach().cpu().numpy()\n",
    "    targets_np = targets.detach().cpu().numpy()\n",
    "    \n",
    "    mse = np.mean((preds_np - targets_np) ** 2)\n",
    "    pearson_corr, _ = pearsonr(preds_np.flatten(), targets_np.flatten())\n",
    "    \n",
    "    return {\n",
    "        'mse': float(mse),\n",
    "        'pearson': float(pearson_corr)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training params and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim: 513\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_dim = glycan_encoder.embedding_dim + protein_encoder.embedding_dim + 1  # Input dimension (glycan + protein + concentration)\n",
    "print('input dim:', input_dim)\n",
    "# Define output dimensions for each task\n",
    "task_output_dims = {0: 1, 1: 1}  # Task 0 (GlycanML): 1 class regression, Task 1 (469): 1 class regression\n",
    "\n",
    "# Create the multi-task network\n",
    "model = MultiTask_Network(input_dim=input_dim,\n",
    "                          task_output_dims=task_output_dims,\n",
    "                          hidden_dims=[256, 128, 64]).to(device)\n",
    "\n",
    "# Example forward pass for task 0\n",
    "#input_tensor = torch.randn(32, input_dim)  # Example input\n",
    "#task_id = 1\n",
    "#output = model(input_tensor, task_id) # Pass the input and the task id to the model\n",
    "#print(f\"Output for task {task_id}: {output.shape}\")\n",
    "\n",
    "# Loss and activation functions for each task\n",
    "loss_functions = {0: nn.MSELoss(), 1: nn.MSELoss()}   \n",
    "\n",
    "#nn.Sigmoid()\n",
    "activation_functions = {0: None, 1: None} # Task 1 & 2: No activation (regression)\n",
    "\n",
    "loss_weights = {0: 1.0, 1: 1.0}\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    list(glycan_encoder.parameters()) +\n",
    "    list(protein_encoder.parameters()) +\n",
    "    list(model.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "\n",
    "# handles batch interleaving to make sure we dont commit CATASTROPHIC FORGETTING\n",
    "train_loader = CombinedLoader(\n",
    "    {\"glyML\": glyML_train_loader, \"task469\": task469_train_loader},\n",
    "    mode=\"max_size_cycle\"  # Oversample the smaller dataset  # \"min_size\"\n",
    ")\n",
    "\n",
    "# Dont need batch interleaving for validation here as not training\n",
    "val_loader = CombinedLoader(\n",
    "    {\"glyML\": glyML_val_loader, \"task469\": task469_val_loader},\n",
    "    mode=\"max_size_cycle\"  # Also use max_size_cycle for validation\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, model, glycan_encoder, protein_encoder, optimizer, \n",
    "                loss_functions, loss_weights, device):\n",
    "    # Set to training mode\n",
    "    glycan_encoder.train()\n",
    "    protein_encoder.train()\n",
    "    model.train()\n",
    "    \n",
    "    # Setup metrics tracking\n",
    "    task_metrics = {\n",
    "        \"glyML\": {\"total_loss\": 0, \"predictions\": [], \"targets\": []},\n",
    "        \"task469\": {\"total_loss\": 0, \"predictions\": [], \"targets\": []}\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Initialize the iterator for the combined loader\n",
    "    train_iterator = iter(train_loader)\n",
    "    \n",
    "    # Get the total number of batches (now that iterator is initialized)\n",
    "    total_batches = len(train_loader)\n",
    "    \n",
    "    # Progress bar for the combined loader\n",
    "    pbar = tqdm(range(total_batches), desc='Training')\n",
    "    \n",
    "    # Iterate through batches from the combined loader\n",
    "    for _ in pbar:\n",
    "\n",
    "        batch_dict, batch_idx, dataloader_idx = next(train_iterator)\n",
    "        all_tasks_loss = 0\n",
    "        batch_losses = {}\n",
    "        \n",
    "        # Process each task's batch\n",
    "        for task_name, batch in batch_dict.items():\n",
    "            # Get task ID (0 for task469, 1 for glyML)\n",
    "            task_id = batch[\"task_id\"]\n",
    "            # convert that baby to int\n",
    "            task_id = task_id[0].item()\n",
    "            \n",
    "            # Move data to device\n",
    "            glycan_encoding = batch[\"glycan_encoding\"].to(device)\n",
    "            protein_encoding = batch[\"protein_encoding\"].to(device)\n",
    "            concentration = batch[\"concentration\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            \n",
    "            ## Apply log transform if configured\n",
    "            #if config.log_predict:\n",
    "                #targets = torch.log(targets + 1e-6)\n",
    "            \n",
    "            \n",
    "            # Concatenate inputs for the multi-task model\n",
    "            combined_input = torch.cat([glycan_encoding, protein_encoding, concentration], dim=-1)\n",
    "            \n",
    "            # Forward pass through the multi-task model\n",
    "            predictions = model(combined_input, task_id)\n",
    "            \n",
    "            # apply activation func if not None\n",
    "            if activation_functions[task_id] is not None:\n",
    "                predictions = activation_functions[task_id](predictions)\n",
    "                \n",
    "            # Calculate loss\n",
    "            loss_fn = loss_functions[task_id]\n",
    "            loss = loss_fn(predictions, targets) * loss_weights[task_id]\n",
    "            \n",
    "            \n",
    "        \n",
    "            all_tasks_loss += loss\n",
    "            batch_losses[task_name] = loss.item()\n",
    "            \n",
    "            # Revert log transform for metrics calculation\n",
    "            #if config.log_predict:\n",
    "                #predictions = torch.exp(predictions) - 1e-6\n",
    "                #targets = torch.exp(targets) - 1e-6\n",
    "            \n",
    "            \n",
    "            # Store predictions and targets for metrics calculation\n",
    "            task_metrics[task_name][\"predictions\"].append(predictions.detach())\n",
    "            task_metrics[task_name][\"targets\"].append(targets.detach())\n",
    "            task_metrics[task_name][\"total_loss\"] += loss.item()\n",
    "        \n",
    "        # Optimization step for all tasks together\n",
    "        optimizer.zero_grad()\n",
    "        all_tasks_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress bar with current losses\n",
    "        pbar.set_postfix({\n",
    "            f\"{task}_loss\": f\"{loss:.4f}\" for task, loss in batch_losses.items()\n",
    "        })\n",
    "    \n",
    "    # Calculate final metrics for each task\n",
    "    final_metrics = {}\n",
    "    for task_name, data in task_metrics.items():\n",
    "        if data[\"predictions\"]:  # Check if we have any predictions\n",
    "            epoch_predictions = torch.cat(data[\"predictions\"])\n",
    "            epoch_targets = torch.cat(data[\"targets\"])\n",
    "            task_result = calculate_metrics(epoch_predictions, epoch_targets)\n",
    "            task_result[\"loss\"] = data[\"total_loss\"]\n",
    "            final_metrics[task_name] = task_result\n",
    "    \n",
    "    return final_metrics\n",
    "\n",
    "# Define validation epoch function\n",
    "def validate_epoch(val_loader, model, glycan_encoder, protein_encoder, \n",
    "                  loss_functions, loss_weights, device):\n",
    "    # Set to evaluation mode\n",
    "    glycan_encoder.eval()\n",
    "    protein_encoder.eval()\n",
    "    model.eval()\n",
    "    \n",
    "    # Setup metrics tracking\n",
    "    task_metrics = {\n",
    "        \"glyML\": {\"total_loss\": 0, \"predictions\": [], \"targets\": []},\n",
    "        \"task469\": {\"total_loss\": 0, \"predictions\": [], \"targets\": []}\n",
    "    }\n",
    "    \n",
    "    # Initialize the iterator for the combined loader\n",
    "    val_iterator = iter(val_loader)\n",
    "    \n",
    "    # Get the total number of batches (now that iterator is initialized)\n",
    "    total_batches = len(val_loader)\n",
    "    \n",
    "    # Progress bar for the combined loader\n",
    "    pbar = tqdm(range(total_batches), desc='Validating')\n",
    "    \n",
    "    \n",
    "    #with torch.no_grad():\n",
    "    # Iterate through batches from the combined loader\n",
    "    for _ in pbar:\n",
    "        batch_dict, batch_idx, dataloader_idx = next(val_iterator)\n",
    "        batch_losses = {}\n",
    "        \n",
    "        # Process each task's batch\n",
    "        for task_name, batch in batch_dict.items():\n",
    "            # Get task ID\n",
    "            task_id = batch[\"task_id\"]\n",
    "            \n",
    "            task_id = task_id[0].item()\n",
    "            \n",
    "            # Move data to device\n",
    "            glycan_encoding = batch[\"glycan_encoding\"].to(device)\n",
    "            protein_encoding = batch[\"protein_encoding\"].to(device)\n",
    "            concentration = batch[\"concentration\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "            \n",
    "            # Apply log transform if configured\n",
    "            #if config.log_predict:\n",
    "                #targets = torch.log(targets + 1e-6)\n",
    "            \n",
    "\n",
    "            \n",
    "            # Concatenate inputs for the multi-task model\n",
    "            combined_input = torch.cat([glycan_encoding, protein_encoding, concentration], dim=-1)\n",
    "            \n",
    "            # Forward pass through the multi-task model\n",
    "            predictions = model(combined_input, task_id)\n",
    "            \n",
    "            # apply activation func if not None\n",
    "            if activation_functions[task_id] is not None:\n",
    "                predictions = activation_functions[task_id](predictions)\n",
    "                \n",
    "            # Calculate loss\n",
    "            loss_fn = loss_functions[task_id]\n",
    "            loss = loss_fn(predictions, targets) * loss_weights[task_id]\n",
    "            \n",
    "            # Revert log transform for metrics calculation\n",
    "            #if config.log_predict:\n",
    "                #predictions = torch.exp(predictions) - 1e-6\n",
    "                #targets = torch.exp(targets) - 1e-6\n",
    "\n",
    "            # Store predictions and targets for metrics calculation\n",
    "            task_metrics[task_name][\"predictions\"].append(predictions.detach())\n",
    "            task_metrics[task_name][\"targets\"].append(targets.detach())\n",
    "            task_metrics[task_name][\"total_loss\"] += loss.item()\n",
    "        \n",
    "        # Update progress bar with current losses\n",
    "        pbar.set_postfix({\n",
    "            f\"{task}_loss\": f\"{loss:.4f}\" for task, loss in batch_losses.items()\n",
    "        })\n",
    "    \n",
    "    # Calculate final metrics for each task\n",
    "    final_metrics = {}\n",
    "    for task_name, data in task_metrics.items():\n",
    "        if data[\"predictions\"]:  # Check if we have any predictions\n",
    "            epoch_predictions = torch.cat(data[\"predictions\"])\n",
    "            epoch_targets = torch.cat(data[\"targets\"])\n",
    "            task_result = calculate_metrics(epoch_predictions, epoch_targets)\n",
    "            task_result[\"loss\"] = data[\"total_loss\"]\n",
    "            final_metrics[task_name] = task_result\n",
    "    \n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate(val_loader: DataLoader, task_id: int, device) -> Dict[str, float]:\n",
    "        # Set to evaluation mode\n",
    "    glycan_encoder.eval()\n",
    "    protein_encoder.eval()\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validating')\n",
    "        for batch in pbar:\n",
    "            glycan_encoding = batch['glycan_encoding'].to(device)\n",
    "            protein_encoding = batch['protein_encoding'].to(device)\n",
    "            concentration = batch['concentration'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            #if self.config.log_predict:\n",
    "                #targets = torch.log(targets + 1e-6) #torch.log1p(targets)\n",
    "            \n",
    "            # Concatenate inputs for the multi-task model\n",
    "            combined_input = torch.cat([glycan_encoding, protein_encoding, concentration], dim=-1)\n",
    "            \n",
    "            # Forward pass through the multi-task model\n",
    "            predictions = model(combined_input, task_id)\n",
    "            \n",
    "            # apply activation func if not None\n",
    "            if activation_functions[task_id] is not None:\n",
    "                predictions = activation_functions[task_id](predictions)\n",
    "                \n",
    "            # Calculate loss\n",
    "            loss_fn = loss_functions[task_id]\n",
    "            loss = loss_fn(predictions, targets) * loss_weights[task_id]\n",
    "            \n",
    "            # track totals\n",
    "            total_loss += loss.item()\n",
    "            all_predictions.append(predictions)\n",
    "            all_targets.append(targets)\n",
    "            \n",
    "            # Update progress bar with current loss\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    # save metrics\n",
    "    val_predictions = torch.cat(all_predictions)\n",
    "    val_targets = torch.cat(all_targets)\n",
    "    metrics = calculate_metrics(val_predictions, val_targets)\n",
    "    metrics['loss'] = total_loss #/ len(val_loader)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5899/5899 [01:41<00:00, 58.18it/s, glyML_loss=0.6733, task469_loss=0.0134] \n",
      "Validating: 100%|| 252/252 [00:01<00:00, 188.88it/s, loss=0.0069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "  glyML: mse: 0.7382, pearson: 0.0182, loss: 4354.5588\n",
      "  task469: mse: 0.0106, pearson: 0.0279, loss: 62.7439\n",
      "\n",
      "\n",
      "{'mse': 0.004806539509445429, 'pearson': 0.07678210735321045, 'loss': 1.2118915317114443}\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 5899/5899 [01:38<00:00, 59.74it/s, glyML_loss=0.2649, task469_loss=0.0147] \n",
      "Validating: 100%|| 252/252 [00:01<00:00, 204.87it/s, loss=0.0018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "  glyML: mse: 0.7257, pearson: 0.0672, loss: 4281.0098\n",
      "  task469: mse: 0.0065, pearson: 0.1048, loss: 38.3662\n",
      "\n",
      "\n",
      "{'mse': 0.0048169647343456745, 'pearson': 0.16498017311096191, 'loss': 1.212919050711207}\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|     | 2740/5899 [00:45<01:03, 49.42it/s, glyML_loss=2.5917, task469_loss=0.0052] "
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    train_metrics = train_epoch(\n",
    "        train_loader, model, glycan_encoder, protein_encoder, optimizer, \n",
    "        loss_functions, loss_weights, device\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Validation phase\n",
    "    #val_metrics = validate_epoch(\n",
    "        #val_loader, model, glycan_encoder, protein_encoder, \n",
    "        #loss_functions, loss_weights, device\n",
    "    #)\n",
    "    \n",
    "    val_metr = _validate(task469_val_loader, task_id=0, device=device)\n",
    "    \n",
    "    \n",
    "    print(f\"Training Results:\")\n",
    "    for task_name, metrics in train_metrics.items():\n",
    "        metrics_str = \", \".join([f\"{k}: {v:.4f}\" for k, v in metrics.items()])\n",
    "        print(f\"  {task_name}: {metrics_str}\")\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(val_metr)\n",
    "    \n",
    "    \n",
    "    #print(f\"Validation Results:\")\n",
    "    #for task_name, metrics in val_metrics.items():\n",
    "        #metrics_str = \", \".join([f\"{k}: {v:.4f}\" for k, v in metrics.items()])\n",
    "        #print(f\"  {task_name}: {metrics_str}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
