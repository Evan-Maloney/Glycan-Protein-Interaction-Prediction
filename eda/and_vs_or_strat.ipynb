{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_glycans(glycans, radius, fp_size, n_clusters):\n",
    "\n",
    "    def get_morgan_count_fingerprint(smiles, radius, fp_size):\n",
    "        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return {f\"mf_{i}\": 0 for i in range(fp_size)} \n",
    "\n",
    "\n",
    "        #The useChirality parameter in Morgan fingerprints determines whether chirality is considered when encoding a molecule.\n",
    "        #includeChirality=True = Differentiates between enantiomers (model will treat mirror-image molecules as different)\n",
    "        #includeChirality=False = Ignores chirality (model will treat mirror-image molecules as the same)\n",
    "        kid_named_morgan_finger = rdFingerprintGenerator.GetMorganGenerator(radius=radius,fpSize=fp_size, includeChirality=True)\n",
    "\n",
    "        cfp = kid_named_morgan_finger.GetCountFingerprint(mol)  \n",
    "        bit_counts = cfp.GetNonzeroElements()  \n",
    "\n",
    "        # Convert to a full fp_size-length feature vector\n",
    "        fingerprint_vector = {f\"mf_{i}\": bit_counts.get(i, 0) for i in range(fp_size)}\n",
    "        return fingerprint_vector\n",
    "\n",
    "    fingerprint_df = glycans['SMILES'].apply(lambda x: get_morgan_count_fingerprint(x, radius, fp_size)).apply(pd.Series)\n",
    "    \n",
    "    glycans = pd.concat([glycans, fingerprint_df], axis=1)\n",
    "    \n",
    "    # matrix version of fingerprint features. Each row is a glycan, each column is a fingerprint component shape: (611, 2048)\n",
    "    finger_counts_matrix = fingerprint_df.values\n",
    "    # pdist calculates the euclidean distance between the combination of each glycan with every other glycan. Then squareform() turns this into a matrix representation where each row is a glycan and each column is the same list of glycans so we can have a comparison matrix. Shape: (611, 611)\n",
    "    dist_matrix = squareform(pdist(finger_counts_matrix, metric=\"euclidean\"))\n",
    "    \n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    labels = kmeans.fit_predict(dist_matrix)\n",
    "    \n",
    "    glycans['cluster_label'] = labels\n",
    "    \n",
    "    return glycans\n",
    "\n",
    "def cluster_proteins(proteins, n_clusters):\n",
    "    \n",
    "    \n",
    "    def compute_protein_features(seq):\n",
    "\n",
    "        # Add reasoning for feature vectors\n",
    "        \n",
    "        # Protein Analysis is a Tool from Biopython\n",
    "        analysis = ProteinAnalysis(seq)\n",
    "        features = {}\n",
    "        \n",
    "        # The following are Basic Features\n",
    "        features['length'] = len(seq)\n",
    "        features['mw'] = analysis.molecular_weight()\n",
    "        features['instability_index'] = analysis.instability_index()\n",
    "\n",
    "        features['net_charge_pH7'] = analysis.charge_at_pH(7.0)\n",
    "\n",
    "        aa_percent = analysis.get_amino_acids_percent()\n",
    "\n",
    "        # Prompted ChatGPT to ask how to parse a\n",
    "        # N, Q, S, T: Polar Amino Acids, often involved in hydrogen bonding with glycans\n",
    "        # K, R: Basic Amino Acids, can form hydrogen bonds and electrostatic bonds\n",
    "        # D, E: Acidic Amino Acids, can interact with positively charged groups of glycans\n",
    "        for aa in ['N', 'Q', 'S', 'T', 'K', 'R', 'D', 'E']:\n",
    "            features[f'frac_{aa}'] = aa_percent.get(aa, 0.0)\n",
    "\n",
    "    \n",
    "    # F, Y, W are aromatic amino acids which bind with glycans\n",
    "        for aa in ['F', 'Y', 'W']:\n",
    "            features[f'frac_{aa}'] = aa_percent.get(aa, 0.0)\n",
    "            features['aromatic_binding_score'] = (\n",
    "            aa_percent.get('F', 0.0) +\n",
    "            aa_percent.get('Y', 0.0) +\n",
    "            aa_percent.get('W', 0.0)\n",
    "        )\n",
    "\n",
    "        features['aromaticity'] = analysis.aromaticity()\n",
    "\n",
    "        features['hydrophobicity'] = analysis.gravy()\n",
    "\n",
    "        return features\n",
    "\n",
    "    feature_dicts = proteins['Amino Acid Sequence'].apply(compute_protein_features)\n",
    "    features_df = pd.DataFrame(list(feature_dicts))\n",
    "\n",
    "    proteins = pd.concat([proteins, features_df], axis=1)\n",
    "    \n",
    "    # Select the feature columns (all columns from the feature extraction)\n",
    "    feature_columns = features_df.columns.tolist()\n",
    "    feature_data = proteins[feature_columns].values\n",
    "\n",
    "    # apply k means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    proteins['cluster_label'] = kmeans.fit_predict(feature_data)\n",
    "    \n",
    "    return proteins\n",
    "\n",
    "def stratified_kfold_split(fractions_df, glycans_df, proteins_df, n_splits, random_state):\n",
    "    \"\"\"\n",
    "    Create a stratified k-fold split where each fold:\n",
    "    1. Contains unique GlycanIDs and ProteinGroups not seen in training\n",
    "    2. Maintains the distribution of cluster_labels for both glycans and proteins\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fractions_df : pandas.DataFrame\n",
    "        DataFrame containing ['ObjId', 'ProteinGroup', 'Concentration', 'GlycanID', 'f']\n",
    "    glycans_df : pandas.DataFrame\n",
    "        DataFrame containing ['Name', 'cluster_label'] where Name maps to GlycanID\n",
    "    proteins_df : pandas.DataFrame\n",
    "        DataFrame containing ['ProteinGroup', 'cluster_label']\n",
    "    n_splits : int, default=5\n",
    "        Number of folds for cross-validation\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fold_indices : list of tuples\n",
    "        List of (train_indices, test_indices) pairs for each fold\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Initialize a list to store fold indices\n",
    "    fold_indices = []\n",
    "    \n",
    "    # Create folds for glycans - using glycans_df directly as it contains unique glycan IDs\n",
    "    # glycan_folds = {0: [[0:20], [20:40], ...], 1: [[0:10], [10:20], ...], 2: [...]}\n",
    "    glycan_folds = {}\n",
    "    for cluster in glycans_df['cluster_label'].unique():\n",
    "        cluster_glycans = glycans_df[glycans_df['cluster_label'] == cluster]['Name'].tolist()\n",
    "        np.random.shuffle(cluster_glycans)\n",
    "        \n",
    "        # Create approximately equal sized groups\n",
    "        glycan_folds[cluster] = []\n",
    "        for i in range(n_splits):\n",
    "            start_idx = int(i * len(cluster_glycans) / n_splits)\n",
    "            end_idx = int((i + 1) * len(cluster_glycans) / n_splits)\n",
    "            glycan_folds[cluster].append(cluster_glycans[start_idx:end_idx])\n",
    "    \n",
    "    # Create folds for proteins - using proteins_df directly as it contains unique protein IDs\n",
    "    protein_folds = {}\n",
    "    for cluster in proteins_df['cluster_label'].unique():\n",
    "        cluster_proteins = proteins_df[proteins_df['cluster_label'] == cluster]['ProteinGroup'].tolist()\n",
    "        np.random.shuffle(cluster_proteins)\n",
    "        \n",
    "        # Create approximately equal sized groups\n",
    "        protein_folds[cluster] = []\n",
    "        for i in range(n_splits):\n",
    "            start_idx = int(i * len(cluster_proteins) / n_splits)\n",
    "            end_idx = int((i + 1) * len(cluster_proteins) / n_splits)\n",
    "            protein_folds[cluster].append(cluster_proteins[start_idx:end_idx])\n",
    "    \n",
    "    # for each fold: 0, 1, 2, ... k-1 (k iterations)\n",
    "    for fold_idx in range(n_splits):\n",
    "        # Collect test glycans and proteins for this fold\n",
    "        # test_glycans = [cluster_0_fold_fold_idx + cluster_1_fold_fold_idx + cluster_2_fold_fold_idx]\n",
    "        test_glycans = []\n",
    "        for cluster, fold_lists in glycan_folds.items():\n",
    "            test_glycans.extend(fold_lists[fold_idx])\n",
    "        \n",
    "        test_proteins = []\n",
    "        for cluster, fold_lists in protein_folds.items():\n",
    "            test_proteins.extend(fold_lists[fold_idx])\n",
    "        \n",
    "        # if one of the test_glycans OR one of the test_proteins is in this sample then put it in test, otherwise put it in train\n",
    "        # becuase of this functionality we need a larger k_fold (something like 8) to get a test_size of around 20% as the OR operation grabs a lot of samples if the test_glycans and test_proteins is high\n",
    "        # ex: k_fold=2: test_glycans=[50% of our glycans], test_proteins=[50% of our proteins] --> 50% of glycans OR 50% of proteins ~= 80% samples. (This creates a test set of size 80%)\n",
    "        is_test = ((fractions_df['GlycanID'].isin(test_glycans)) | \n",
    "                   (fractions_df['ProteinGroup'].isin(test_proteins)))\n",
    "        \n",
    "        test_indices = fractions_df[is_test].index\n",
    "        train_indices = fractions_df[~is_test].index\n",
    "        \n",
    "        fold_indices.append((train_indices, test_indices))\n",
    "    \n",
    "    \n",
    "    # fold_indicies at K_fold=2 = [\n",
    "    #   fold_1: (train_indicies, test_indicies), -> (the_rest, [1/k*total_glycans OR 1/k*total_proteins]) (first half of proteins and glycans in test set)\n",
    "    #   fold_2: (train_indicies, test_indicies), -> (the_rest, [1/k*total_glycans OR 1/k*total_proteins]) (SECOND half of proteins and glycans in test set)\n",
    "    #]\n",
    "    \n",
    "    return fold_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "glycans_df = pd.read_csv('../data/Glycan-Structures-CFG611.txt', sep=\"\\t\")\n",
    "proteins_df = pd.read_csv('../data/Protein-Sequence-Table.txt', sep='\\t')\n",
    "fractions_df = pd.read_csv('../pipeline/data/Train_Fractions.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.435205128148839e-06)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(list(fractions_df['f']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Validation K-fold split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation OR operation K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/test_env/lib/python3.12/site-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "radius = 3\n",
    "fp_size = 1024\n",
    "n_clusters = 3\n",
    "glycans_df = cluster_glycans(glycans_df, radius, fp_size, n_clusters)\n",
    "\n",
    "n_protein_clusters = 3\n",
    "proteins_df = cluster_proteins(proteins_df, n_protein_clusters)\n",
    "\n",
    "k_folds = 8\n",
    "random_state = 42\n",
    "\n",
    "# need to cluster both glycans and proteins so that we can create a stratified k-fold split for training \n",
    "full_indicies_OR = stratified_kfold_split(fractions_df, glycans_df, proteins_df, k_folds, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 64325 samples: (93.92%), validating with 4167 samples: (6.08%) \n",
      "Training with 62396 samples: (91.10%), validating with 6096 samples: (8.90%) \n",
      "Training with 54564 samples: (79.66%), validating with 13928 samples: (20.34%) \n",
      "Training with 44751 samples: (65.34%), validating with 23741 samples: (34.66%) \n",
      "Training with 51055 samples: (74.54%), validating with 17437 samples: (25.46%) \n",
      "Training with 49121 samples: (71.72%), validating with 19371 samples: (28.28%) \n",
      "Training with 48222 samples: (70.41%), validating with 20270 samples: (29.59%) \n",
      "Training with 46298 samples: (67.60%), validating with 22194 samples: (32.40%) \n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_idx, test_idx) in enumerate(full_indicies_OR):\n",
    "    # Get data for this fold\n",
    "    train_data = fractions_df.loc[train_idx]\n",
    "    val_data = fractions_df.loc[test_idx]\n",
    "    \n",
    "    print(f\"Training with {len(train_data)} samples: ({(len(train_data) / (len(train_data) + len(val_data))) * 100:.2f}%), \"\n",
    "                  f\"validating with {len(val_data)} samples: ({(len(val_data) / (len(train_data) + len(val_data))) * 100:.2f}%) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation AND Operation K-fold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 58265, test size: 84, total: 68492\n",
      "train size: 85.07%, test size: 0.12%\n",
      "train size: 33787, test size: 5960, total: 68492\n",
      "train size: 49.33%, test size: 8.7%\n",
      "train size: 34496, test size: 5624, total: 68492\n",
      "train size: 50.37%, test size: 8.21%\n",
      "train size: 29955, test size: 7851, total: 68492\n",
      "train size: 43.74%, test size: 11.46%\n"
     ]
    }
   ],
   "source": [
    "n_splits = 4\n",
    "np.random.seed(random_state)\n",
    "    \n",
    "# Initialize a list to store fold indices\n",
    "fold_indices_AND = []\n",
    "\n",
    "# Create folds for glycans - using glycans_df directly as it contains unique glycan IDs\n",
    "# glycan_folds = {0: [[0:20], [20:40], ...], 1: [[0:10], [10:20], ...], 2: [...]}\n",
    "glycan_folds = {}\n",
    "for cluster in glycans_df['cluster_label'].unique():\n",
    "    cluster_glycans = glycans_df[glycans_df['cluster_label'] == cluster]['Name'].tolist()\n",
    "    np.random.shuffle(cluster_glycans)\n",
    "    \n",
    "    # Create approximately equal sized groups\n",
    "    glycan_folds[cluster] = []\n",
    "    for i in range(n_splits):\n",
    "        start_idx = int(i * len(cluster_glycans) / n_splits)\n",
    "        end_idx = int((i + 1) * len(cluster_glycans) / n_splits)\n",
    "        glycan_folds[cluster].append(cluster_glycans[start_idx:end_idx])\n",
    "\n",
    "# Create folds for proteins - using proteins_df directly as it contains unique protein IDs\n",
    "protein_folds = {}\n",
    "for cluster in proteins_df['cluster_label'].unique():\n",
    "    cluster_proteins = proteins_df[proteins_df['cluster_label'] == cluster]['ProteinGroup'].tolist()\n",
    "    np.random.shuffle(cluster_proteins)\n",
    "    \n",
    "    # Create approximately equal sized groups\n",
    "    protein_folds[cluster] = []\n",
    "    for i in range(n_splits):\n",
    "        start_idx = int(i * len(cluster_proteins) / n_splits)\n",
    "        end_idx = int((i + 1) * len(cluster_proteins) / n_splits)\n",
    "        protein_folds[cluster].append(cluster_proteins[start_idx:end_idx])\n",
    "        \n",
    "for fold_idx in range(n_splits):\n",
    "        # Collect test glycans and proteins for this fold\n",
    "        # test_glycans = [cluster_0_fold_fold_idx + cluster_1_fold_fold_idx + cluster_2_fold_fold_idx]\n",
    "        test_glycans = []\n",
    "        for cluster, fold_lists in glycan_folds.items():\n",
    "            test_glycans.extend(fold_lists[fold_idx])\n",
    "        \n",
    "        test_proteins = []\n",
    "        for cluster, fold_lists in protein_folds.items():\n",
    "            test_proteins.extend(fold_lists[fold_idx])\n",
    "        \n",
    "        # if one of the test_glycans OR one of the test_proteins is in this sample then put it in test, otherwise put it in train\n",
    "        # becuase of this functionality we need a larger k_fold (something like 8) to get a test_size of around 20% as the OR operation grabs a lot of samples if the test_glycans and test_proteins is high\n",
    "        # ex: k_fold=2: test_glycans=[50% of our glycans], test_proteins=[50% of our proteins] --> 50% of glycans OR 50% of proteins ~= 80% samples. (This creates a test set of size 80%)\n",
    "        is_test = ((fractions_df['GlycanID'].isin(test_glycans)) & \n",
    "                   (fractions_df['ProteinGroup'].isin(test_proteins)))\n",
    "        \n",
    "        is_train = ((~fractions_df['GlycanID'].isin(test_glycans)) & \n",
    "                   (~fractions_df['ProteinGroup'].isin(test_proteins)))\n",
    "        \n",
    "        test_indices = fractions_df[is_test].index\n",
    "        #not_test_indices = fractions_df[~is_test].index\n",
    "        \n",
    "        train_indices = fractions_df[is_train].index\n",
    "        \n",
    "        print(f'train size: {len(train_indices)}, test size: {len(test_indices)}, total: {len(fractions_df)}')\n",
    "        \n",
    "        print(f'train size: {round((len(train_indices)/len(fractions_df))*100, 2)}%, test size: {round((len(test_indices)/len(fractions_df))*100, 2)}%')\n",
    "        \n",
    "        fold_indices_AND.append((train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 fold (50% of each class)\n",
    "\n",
    "    train size: 28149, test size: 26509, total: 109768\n",
    "\n",
    "    train size: 25.64%, test size: 24.15%\n",
    "\n",
    "\n",
    "\n",
    "3 fold (33% of each glycan and protein classes)\n",
    "\n",
    "    train size: 53445, test size: 9950, total: 109768\n",
    "\n",
    "    train size: 48.69%, test size: 9.06%\n",
    "\n",
    "\n",
    "4 fold (25%)\n",
    "\n",
    "    train size: 67441, test size: 5029, total: 109768\n",
    "\n",
    "    train size: 61.44%, test size: 4.58%\n",
    "\n",
    "5 fold (20%)\n",
    "\n",
    "    train size: 71899, test size: 3904, total: 109768\n",
    "    \n",
    "    train size: 65.5%, test size: 3.56%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 75918 samples: (96.24%), validating with 2964 samples: (3.76%) \n",
      "Training with 66597 samples: (92.54%), validating with 5368 samples: (7.46%) \n",
      "Training with 71899 samples: (94.85%), validating with 3904 samples: (5.15%) \n",
      "Training with 71903 samples: (94.85%), validating with 3904 samples: (5.15%) \n",
      "Training with 64952 samples: (91.77%), validating with 5825 samples: (8.23%) \n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_idx, test_idx) in enumerate(fold_indices_AND):\n",
    "    # Get data for this fold\n",
    "    train_data = fractions_df.loc[train_idx]\n",
    "    val_data = fractions_df.loc[test_idx]\n",
    "    \n",
    "    print(f\"Training with {len(train_data)} samples: ({(len(train_data) / (len(train_data) + len(val_data))) * 100:.2f}%), \"\n",
    "                  f\"validating with {len(val_data)} samples: ({(len(val_data) / (len(train_data) + len(val_data))) * 100:.2f}%) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Test size: 10.0% -------------\n",
      "train size: 99882, test size: 1860, total: 129489\n",
      "train size: 77.14%, test size: 1.44%\n",
      "test size % in terms of training+test set size: 1.83%\n",
      "Total % of dataset used: 78.57%\n",
      "\n",
      "-------------Test size: 20.0% -------------\n",
      "train size: 80842, test size: 5697, total: 129489\n",
      "train size: 62.43%, test size: 4.4%\n",
      "test size % in terms of training+test set size: 6.58%\n",
      "Total % of dataset used: 66.83%\n",
      "\n",
      "-------------Test size: 25.0% -------------\n",
      "train size: 69436, test size: 9240, total: 129489\n",
      "train size: 53.62%, test size: 7.14%\n",
      "test size % in terms of training+test set size: 11.74%\n",
      "Total % of dataset used: 60.76%\n",
      "\n",
      "-------------Test size: 27.0% -------------\n",
      "train size: 66156, test size: 10506, total: 129489\n",
      "train size: 51.09%, test size: 8.11%\n",
      "test size % in terms of training+test set size: 13.7%\n",
      "Total % of dataset used: 59.2%\n",
      "\n",
      "-------------Test size: 30.0% -------------\n",
      "train size: 63174, test size: 11773, total: 129489\n",
      "train size: 48.79%, test size: 9.09%\n",
      "test size % in terms of training+test set size: 15.71%\n",
      "Total % of dataset used: 57.88%\n",
      "\n",
      "-------------Test size: 40.0% -------------\n",
      "train size: 44140, test size: 22386, total: 129489\n",
      "train size: 34.09%, test size: 17.29%\n",
      "test size % in terms of training+test set size: 33.65%\n",
      "Total % of dataset used: 51.38%\n",
      "\n",
      "-------------Test size: 50.0% -------------\n",
      "train size: 31717, test size: 33028, total: 129489\n",
      "train size: 24.49%, test size: 25.51%\n",
      "test size % in terms of training+test set size: 51.01%\n",
      "Total % of dataset used: 50.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "fractions_df = pd.read_csv('../data/Fractions-Bound-Table.txt', sep='\\t')\n",
    "\n",
    "np.random.seed(random_state)\n",
    "\n",
    "    \n",
    "# Filter out rows where ProteinGroup is not in proteins_df\n",
    "valid_protein_groups = set(proteins_df['ProteinGroup'])\n",
    "fractions_df = fractions_df[fractions_df['ProteinGroup'].isin(valid_protein_groups)].copy()\n",
    "\n",
    "\n",
    "# Calculate target counts for each cluster in test set\n",
    "glycan_cluster_counts = glycans_df['cluster_label'].value_counts().to_dict()\n",
    "protein_cluster_counts = proteins_df['cluster_label'].value_counts().to_dict()\n",
    "\n",
    "test_sizes = [0.10, 0.20, 0.25, 0.27, 0.30, 0.40, 0.50]\n",
    "\n",
    "#test_sizes = [0.25, 0.27, 0.28, 0.30, 0.33, 0.35]\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    \n",
    "\n",
    "    glycan_test_counts = {cluster: max(1, int(np.ceil(count * test_size))) \n",
    "                            for cluster, count in glycan_cluster_counts.items()}\n",
    "    protein_test_counts = {cluster: max(1, int(np.ceil(count * test_size))) \n",
    "                            for cluster, count in protein_cluster_counts.items()}\n",
    "\n",
    "    # Select glycans and proteins for test set while respecting cluster distributions\n",
    "    test_glycans = []\n",
    "    for cluster, target_count in glycan_test_counts.items():\n",
    "        cluster_glycans = glycans_df[glycans_df['cluster_label'] == cluster]['Name'].tolist()\n",
    "        selected = np.random.choice(cluster_glycans, size=min(target_count, len(cluster_glycans)), replace=False)\n",
    "        test_glycans.extend(selected)\n",
    "\n",
    "    test_proteins = []\n",
    "    for cluster, target_count in protein_test_counts.items():\n",
    "        cluster_proteins = proteins_df[proteins_df['cluster_label'] == cluster]['ProteinGroup'].tolist()\n",
    "        selected = np.random.choice(cluster_proteins, size=min(target_count, len(cluster_proteins)), replace=False)\n",
    "        test_proteins.extend(selected)\n",
    "\n",
    "    # Create train and test masks\n",
    "    is_test = ((fractions_df['GlycanID'].isin(test_glycans)) & \n",
    "                (fractions_df['ProteinGroup'].isin(test_proteins)))\n",
    "\n",
    "    is_train = ((~fractions_df['GlycanID'].isin(test_glycans)) & \n",
    "                    (~fractions_df['ProteinGroup'].isin(test_proteins)))\n",
    "            \n",
    "    test_indices = fractions_df[is_test].index\n",
    "\n",
    "\n",
    "    train_indices = fractions_df[is_train].index\n",
    "    \n",
    "    print(f'-------------Test size: {test_size*100}% -------------')\n",
    "\n",
    "    print(f'train size: {len(train_indices)}, test size: {len(test_indices)}, total: {len(fractions_df)}')\n",
    "            \n",
    "    print(f'train size: {round((len(train_indices)/len(fractions_df))*100, 2)}%, test size: {round((len(test_indices)/len(fractions_df))*100, 2)}%')\n",
    "    \n",
    "    print(f'test size % in terms of test/(training+test) size: {round((len(test_indices)/(len(train_indices)+len(test_indices)))*100, 2)}%')\n",
    "    \n",
    "    print(f'Total % of dataset used: {round(((len(train_indices)+len(test_indices))/len(fractions_df))*100, 2)}%\\n')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
